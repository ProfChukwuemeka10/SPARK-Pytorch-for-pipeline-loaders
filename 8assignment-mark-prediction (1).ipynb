{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "WNHFFv5Haeko"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01moptim\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-05T15:38:01.106354Z",
     "iopub.status.busy": "2025-04-05T15:38:01.105948Z",
     "iopub.status.idle": "2025-04-05T15:38:01.112776Z",
     "shell.execute_reply": "2025-04-05T15:38:01.111607Z",
     "shell.execute_reply.started": "2025-04-05T15:38:01.106320Z"
    },
    "id": "5vap3_N1ae6E"
   },
   "outputs": [],
   "source": [
    "# Assignment marks: training set\n",
    "train_samples_np = np.array([78, 100, 52, 89, 92, 87, 65, 40, 78, 82, 64, 78, 98, 86, 72, 81, 94, 92, 51, 71])\n",
    "train_labels_np = np.array([  1,   1,  0,  1,  1,  1,  0,  0,  1,  1,  0,  1,  1,  1,  0,  1,  1,  1,  0,  0])\n",
    "\n",
    "# Assignment marks: testing set\n",
    "test_samples_np = np.array([75, 68, 99, 82, 71, 70, 68, 84, 87, 72, 61, 92, 93, 54, 63, 45, 74, 76, 83, 91])\n",
    "test_labels_np = np.array([  1,  0,  1,  1,  0,  0,  0,  1,  1,  0,  0,  1,  1,  0,  0,  0,  0,  1,  1,  1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-05T15:42:38.820354Z",
     "iopub.status.busy": "2025-04-05T15:42:38.820022Z",
     "iopub.status.idle": "2025-04-05T15:42:38.824360Z",
     "shell.execute_reply": "2025-04-05T15:42:38.823406Z",
     "shell.execute_reply.started": "2025-04-05T15:42:38.820328Z"
    }
   },
   "outputs": [],
   "source": [
    "# # print(train_samples_np)\n",
    "# for grade in train_samples_np:\n",
    "#     if grade >= 75:\n",
    "#         print(f'{grade}:1')\n",
    "#     else:\n",
    "#         print(f'{grade}:0')\n",
    "        \n",
    "#     # print(grade)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define the Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-05T15:50:48.498490Z",
     "iopub.status.busy": "2025-04-05T15:50:48.498122Z",
     "iopub.status.idle": "2025-04-05T15:50:48.505953Z",
     "shell.execute_reply": "2025-04-05T15:50:48.504838Z",
     "shell.execute_reply.started": "2025-04-05T15:50:48.498463Z"
    },
    "id": "pZTaCq5Kaq7h"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "class SimpleDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A simple dataset for the assignment marks example.\n",
    "    \"\"\"\n",
    "    def __init__(self, samples, labels):\n",
    "        self.samples = torch.tensor(samples, dtype = torch.float32) # Convert samples to float32 tensor and assign to self.samples\n",
    "        self.labels = torch.tensor(labels, dtype = torch.long) # Convert labels to long tensor and assign to self.labels\n",
    "        self.n_samples = len(self.samples) # Store the number of samples in self.n_sample\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the total number of samples\n",
    "        return self.n_samples\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Return the sample and label at the given index\n",
    "        return self.samples[index], self.labels[index]\n",
    "\n",
    "\n",
    "# loader = SimpleDataset(train_samples_np, train_labels_np)\n",
    "# print(loader.n_samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-05T16:18:15.665888Z",
     "iopub.status.busy": "2025-04-05T16:18:15.665543Z",
     "iopub.status.idle": "2025-04-05T16:18:15.673933Z",
     "shell.execute_reply": "2025-04-05T16:18:15.673034Z",
     "shell.execute_reply.started": "2025-04-05T16:18:15.665859Z"
    },
    "id": "DhwR0ziVa6NX"
   },
   "outputs": [],
   "source": [
    "# Rescale the samples to have a mean of 0 and a variance of 1\n",
    "scaler = StandardScaler()  # Initialize the scaler\n",
    "train_samples_scaled = scaler.fit_transform(train_samples_np.reshape(-1,1))  # Fit the scaler on train_samples_np and transform\n",
    "test_samples_scaled = scaler.fit_transform(test_samples_np.reshape(-1,1))   # Transform test_samples_np using the same scaler\n",
    "\n",
    "# Create PyTorch Datasets\n",
    "train_dataset = SimpleDataset(train_samples_scaled, train_labels_np)  # Create SimpleDataset with train_samples_scaled and train_labels_np\n",
    "test_dataset = SimpleDataset(test_samples_scaled, test_labels_np)  # Create SimpleDataset with test_samples_scaled and test_labels_np\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=6, shuffle=True) # Create DataLoader with train_dataset, batch_size=6, shuffle=True\n",
    "test_loader = DataLoader(test_dataset, batch_size=5, shuffle=False)   # Create DataLoader with test_dataset, batch_size=5, shuffle=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define the PyTorch Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-05T16:10:21.194723Z",
     "iopub.status.busy": "2025-04-05T16:10:21.194318Z",
     "iopub.status.idle": "2025-04-05T16:10:21.203407Z",
     "shell.execute_reply": "2025-04-05T16:10:21.202474Z",
     "shell.execute_reply.started": "2025-04-05T16:10:21.194693Z"
    },
    "id": "QAes9yKPbWPK"
   },
   "outputs": [],
   "source": [
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.hidden = nn.Linear(input_size, hidden_size)   # Define a Linear layer from input_size to hidden_size\n",
    "        self.sigmoid = nn.Sigmoid()  # Define a Sigmoid activation\n",
    "        self.output = nn.Linear(hidden_size, output_size)    # Define a Linear layer from hidden_size to output_size\n",
    "        self.softmax = nn.Softmax(dim=1)  # Define a Softmax activation along dimension 1\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.hidden(x)  # Pass x through the hidden layer\n",
    "        x = self.sigmoid(x)  # Apply the sigmoid activation\n",
    "        x = self.output(x)  # Pass through the output layer\n",
    "        x = self.softmax(x)  # Apply softmax to get output probabilities\n",
    "        return x\n",
    "\n",
    "# Instantiate the model\n",
    "input_size = 1\n",
    "hidden_size = 4\n",
    "output_size = 2 # Two output classes\n",
    "model = SimpleNN(input_size, hidden_size, output_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Define Loss Function and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-05T16:10:38.743916Z",
     "iopub.status.busy": "2025-04-05T16:10:38.743534Z",
     "iopub.status.idle": "2025-04-05T16:10:41.266250Z",
     "shell.execute_reply": "2025-04-05T16:10:41.265356Z",
     "shell.execute_reply.started": "2025-04-05T16:10:38.743886Z"
    }
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()  # Define a loss function suitable for multi-class classification (e.g., CrossEntropyLoss)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Define an optimizer (e.g., SGD or Adam) with model parameters and learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-04-05T16:22:08.728457Z",
     "iopub.status.busy": "2025-04-05T16:22:08.728062Z",
     "iopub.status.idle": "2025-04-05T16:22:13.666823Z",
     "shell.execute_reply": "2025-04-05T16:22:13.665918Z",
     "shell.execute_reply.started": "2025-04-05T16:22:08.728424Z"
    },
    "id": "AiKz3UAZbc7D",
    "outputId": "fde3587c-7e75-40f7-d154-a7fd8f0b4056"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [50/1000], Loss: 0.4403\n",
      "Epoch [100/1000], Loss: 0.9745\n",
      "Epoch [150/1000], Loss: 0.4022\n",
      "Epoch [200/1000], Loss: 0.6405\n",
      "Epoch [250/1000], Loss: 0.4193\n",
      "Epoch [300/1000], Loss: 0.4294\n",
      "Epoch [350/1000], Loss: 0.3443\n",
      "Epoch [400/1000], Loss: 0.4640\n",
      "Epoch [450/1000], Loss: 0.3661\n",
      "Epoch [500/1000], Loss: 0.4096\n",
      "Epoch [550/1000], Loss: 0.3375\n",
      "Epoch [600/1000], Loss: 0.3750\n",
      "Epoch [650/1000], Loss: 0.3710\n",
      "Epoch [700/1000], Loss: 0.3648\n",
      "Epoch [750/1000], Loss: 0.4275\n",
      "Epoch [800/1000], Loss: 0.3584\n",
      "Epoch [850/1000], Loss: 0.3207\n",
      "Epoch [900/1000], Loss: 0.3198\n",
      "Epoch [950/1000], Loss: 0.3183\n",
      "Epoch [1000/1000], Loss: 0.4184\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 1000  # Set the number of training epochs\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, labels in train_loader:\n",
    "        #print(inputs[0], labels[0])\n",
    "        \n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass through the model to get outputs\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        \n",
    "        # Compute the loss using criterion\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass (loss.backward)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        # Print epoch number and current loss\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-04-05T16:29:42.047228Z",
     "iopub.status.busy": "2025-04-05T16:29:42.046862Z",
     "iopub.status.idle": "2025-04-05T16:29:42.058566Z",
     "shell.execute_reply": "2025-04-05T16:29:42.057679Z",
     "shell.execute_reply.started": "2025-04-05T16:29:42.047198Z"
    },
    "id": "jLItJfoObiOS",
    "outputId": "3a6b2610-91f3-417d-ae63-0218940d3143"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted labels on testing set: [1 0 1 1 0 0 0 1 1 0 0 1 1 0 0 0 1 1 1 1]\n",
      "True labels on testing set: [1 0 1 1 0 0 0 1 1 0 0 1 1 0 0 0 0 1 1 1]\n",
      "Prediction error on testing set: 5.0\n"
     ]
    }
   ],
   "source": [
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "all_predicted_labels = []\n",
    "all_test_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs) # Perform a forward pass to get outputs\n",
    "\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        all_predicted_labels.extend(predicted.numpy())\n",
    "        all_test_labels.extend(labels.numpy())\n",
    "\n",
    "predicted_labels_np = np.array(all_predicted_labels)  # Convert all_predicted_labels to a NumPy array\n",
    "test_labels_np =  np.array(all_test_labels)       # Convert all_test_labels to a NumPy array\n",
    "\n",
    "# Print predicted and true labels\n",
    "print(\"Predicted labels on testing set:\", predicted_labels_np)\n",
    "print(\"True labels on testing set:\", test_labels_np)\n",
    "\n",
    "# Compute prediction error as a percentage\n",
    "prediction_error_test = np.sum(np.abs(predicted_labels_np - test_labels_np)/len(test_labels_np))*100 # Compute the average absolute error percentage\n",
    "print(\"Prediction error on testing set:\", prediction_error_test)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
